{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaisb\\AppData\\Local\\Temp\\ipykernel_8728\\2116984431.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "d:\\Library\\Downloads\\Documents\\School\\SUPSI_23-24\\hackathon_2\\hackathon-2\\venv\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>proc_text</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>majority_vote</th>\n",
       "      <th>roundID</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fuck you china. I was 2 years clear of severe ...</td>\n",
       "      <td>Fuck you china. I was 2 years clear of severe ...</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>Round1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.92</td>\n",
       "      <td>7.69</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.85</td>\n",
       "      <td>11.54</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feeling so off today and I can’t tell if this ...</td>\n",
       "      <td>Feeling so off today and I can’t tell if this ...</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>Round1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good lord people we need to stop having people...</td>\n",
       "      <td>Good lord people we need to stop having people...</td>\n",
       "      <td>PP</td>\n",
       "      <td>UN</td>\n",
       "      <td>PP</td>\n",
       "      <td>PO</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>Round1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.05</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.52</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm telling yall, Chill the fuck out!! This Vi...</td>\n",
       "      <td>I'm telling yall, Chill the fuck out!! This Vi...</td>\n",
       "      <td>PO</td>\n",
       "      <td>PO</td>\n",
       "      <td>PO</td>\n",
       "      <td>PO</td>\n",
       "      <td>PO</td>\n",
       "      <td>PP</td>\n",
       "      <td>PO</td>\n",
       "      <td>Round1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>37.21</td>\n",
       "      <td>4.65</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.26</td>\n",
       "      <td>2.33</td>\n",
       "      <td>4.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Episode of #CoronaVirus panic. Man took his gl...</td>\n",
       "      <td>Episode of #CoronaVirus panic. Man took his gl...</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>PP</td>\n",
       "      <td>Round1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>20.69</td>\n",
       "      <td>10.34</td>\n",
       "      <td>5.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1.72</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Fuck you china. I was 2 years clear of severe ...   \n",
       "1  Feeling so off today and I can’t tell if this ...   \n",
       "2  Good lord people we need to stop having people...   \n",
       "3  I'm telling yall, Chill the fuck out!! This Vi...   \n",
       "4  Episode of #CoronaVirus panic. Man took his gl...   \n",
       "\n",
       "                                           proc_text  A1  A2  A3  A4  A5  A6  \\\n",
       "0  Fuck you china. I was 2 years clear of severe ...  PP  PP  PP  PP  PP  PP   \n",
       "1  Feeling so off today and I can’t tell if this ...  PP  PP  PP  PP  PP  PP   \n",
       "2  Good lord people we need to stop having people...  PP  UN  PP  PO  PP  PP   \n",
       "3  I'm telling yall, Chill the fuck out!! This Vi...  PO  PO  PO  PO  PO  PP   \n",
       "4  Episode of #CoronaVirus panic. Man took his gl...  PP  PP  PP  PP  PP  PP   \n",
       "\n",
       "  majority_vote roundID  ...  nonflu  filler  AllPunc  Period  Comma  QMark  \\\n",
       "0            PP  Round1  ...     0.0    0.00    26.92    7.69   3.85    0.0   \n",
       "1            PP  Round1  ...     0.0    0.00    28.00   12.00   0.00    0.0   \n",
       "2            PP  Round1  ...     0.0    0.00    19.05    4.76   0.00    0.0   \n",
       "3            PO  Round1  ...     0.0    0.00    37.21    4.65   2.33    0.0   \n",
       "4            PP  Round1  ...     0.0    1.72    20.69   10.34   5.17    0.0   \n",
       "\n",
       "   Exclam  Apostro  OtherP  Emoji  \n",
       "0    0.00     3.85   11.54    0.0  \n",
       "1    0.00     8.00    8.00   16.0  \n",
       "2    0.00     4.76    9.52    0.0  \n",
       "3   23.26     2.33    4.65    0.0  \n",
       "4    0.00     3.45    1.72    0.0  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'data/r1_r2_annotations_liwc_h.xlsx'\n",
    "dfh = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "\n",
    "dfh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text' 'proc_text' 'A1' 'A2' 'A3' 'A4' 'A5' 'A6' 'majority_vote'\n",
      " 'roundID' 'Segment' 'WC' 'Analytic' 'Clout' 'Authentic' 'Tone' 'WPS'\n",
      " 'BigWords' 'Dic' 'Linguistic' 'function' 'pronoun' 'ppron' 'i' 'we' 'you'\n",
      " 'shehe' 'they' 'ipron' 'det' 'article' 'number' 'prep' 'auxverb' 'adverb'\n",
      " 'conj' 'negate' 'verb' 'adj' 'quantity' 'Drives' 'affiliation' 'achieve'\n",
      " 'power' 'Cognition' 'allnone' 'cogproc' 'insight' 'cause' 'discrep'\n",
      " 'tentat' 'certitude' 'differ' 'memory' 'Affect' 'tone_pos' 'tone_neg'\n",
      " 'emotion' 'emo_pos' 'emo_neg' 'emo_anx' 'emo_anger' 'emo_sad' 'swear'\n",
      " 'Social' 'socbehav' 'prosocial' 'polite' 'conflict' 'moral' 'comm'\n",
      " 'socrefs' 'family' 'friend' 'female' 'male' 'Culture' 'politic'\n",
      " 'ethnicity' 'tech' 'Lifestyle' 'leisure' 'home' 'work' 'money' 'relig'\n",
      " 'Physical' 'health' 'illness' 'wellness' 'mental' 'substances' 'sexual'\n",
      " 'food' 'death' 'need' 'want' 'acquire' 'lack' 'fulfill' 'fatigue'\n",
      " 'reward' 'risk' 'curiosity' 'allure' 'Perception' 'attention' 'motion'\n",
      " 'space' 'visual' 'auditory' 'feeling' 'time' 'focuspast' 'focuspresent'\n",
      " 'focusfuture' 'Conversation' 'netspeak' 'assent' 'nonflu' 'filler'\n",
      " 'AllPunc' 'Period' 'Comma' 'QMark' 'Exclam' 'Apostro' 'OtherP' 'Emoji']\n"
     ]
    }
   ],
   "source": [
    "print(dfh.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh.drop(['A1','A2','A3','A4','A5','A6'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_vote\n",
      "PO    160\n",
      "UN     98\n",
      "PP     95\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['text', 'proc_text', 'majority_vote', 'roundID', 'Segment', 'WC',\n",
       "       'Analytic', 'Clout', 'Authentic', 'Tone', 'WPS', 'BigWords', 'Dic',\n",
       "       'Linguistic', 'function', 'pronoun', 'ppron', 'i', 'we', 'you',\n",
       "       'shehe', 'they', 'ipron', 'det', 'article', 'number', 'prep',\n",
       "       'auxverb', 'adverb', 'conj', 'negate', 'verb', 'adj', 'quantity',\n",
       "       'Drives', 'affiliation', 'achieve', 'power', 'Cognition',\n",
       "       'allnone', 'cogproc', 'insight', 'cause', 'discrep', 'tentat',\n",
       "       'certitude', 'differ', 'memory', 'Affect', 'tone_pos', 'tone_neg',\n",
       "       'emotion', 'emo_pos', 'emo_neg', 'emo_anx', 'emo_anger', 'emo_sad',\n",
       "       'swear', 'Social', 'socbehav', 'prosocial', 'polite', 'conflict',\n",
       "       'moral', 'comm', 'socrefs', 'family', 'friend', 'female', 'male',\n",
       "       'Culture', 'politic', 'ethnicity', 'tech', 'Lifestyle', 'leisure',\n",
       "       'home', 'work', 'money', 'relig', 'Physical', 'health', 'illness',\n",
       "       'wellness', 'mental', 'substances', 'sexual', 'food', 'death',\n",
       "       'need', 'want', 'acquire', 'lack', 'fulfill', 'fatigue', 'reward',\n",
       "       'risk', 'curiosity', 'allure', 'Perception', 'attention', 'motion',\n",
       "       'space', 'visual', 'auditory', 'feeling', 'time', 'focuspast',\n",
       "       'focuspresent', 'focusfuture', 'Conversation', 'netspeak',\n",
       "       'assent', 'nonflu', 'filler', 'AllPunc', 'Period', 'Comma',\n",
       "       'QMark', 'Exclam', 'Apostro', 'OtherP', 'Emoji'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfh = dfh[dfh['majority_vote'] != \"NoMajority\"]\n",
    "dfh.reset_index(drop=True, inplace=True)\n",
    "print(dfh.majority_vote.value_counts())\n",
    "dfh.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dfh['majority_vote']\n",
    "dfh.drop(['majority_vote'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfh[\"proc_text\"], target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorizing the tweets\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Generating TF-IDF vectors for visualization\u001b[39;00m\n\u001b[0;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m dfh[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproc_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generating TF-IDF vectors for visualization\n",
    "texts = dfh['proc_text'].values\n",
    "tfidf_matrix = X_train_counts\n",
    "\n",
    "# Converting TF-IDF matrix to dense format and getting feature names\n",
    "tfidf_dense = tfidf_matrix.todense()\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Creating a DataFrame for easier manipulation\n",
    "df_tfidf = pd.DataFrame(tfidf_dense, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Summarizing the TF-IDF scores for each word across all documents to rank them\n",
    "word_tfidf_sum = df_tfidf.sum().sort_values(ascending=False)\n",
    "\n",
    "print(word_tfidf_sum.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# heatmap\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# sns.heatmap(df_tfidf.iloc[:, :40], cmap='YlGnBu')\n",
    "# plt.title('TF-IDF Scores Heatmap for Top 30 Words in All Documents')\n",
    "# plt.xlabel('Top 30 Words')\n",
    "# plt.ylabel('Documents')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# word cloud\n",
    "combined_texts = ' '.join(texts)\n",
    "wordcloud = WordCloud(background_color='white', max_words=200).generate(combined_texts)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of TF-IDF Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_counts, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_counts)\n",
    "print(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Applying Bag of Words model on the processed text\n",
    "texts = dfh['proc_text'].values\n",
    "count_vect = CountVectorizer(stop_words='english')\n",
    "bow_matrix = count_vect.fit_transform(texts)\n",
    "\n",
    "# Converting BoW matrix to DataFrame for easier visualization\n",
    "feature_names_bow = count_vect.get_feature_names_out()\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=feature_names_bow)\n",
    "\n",
    "# Displaying the shape of the BoW matrix and the first few rows of the DataFrame\n",
    "df_bow_shape = df_bow.shape\n",
    "print('Shape of Bag of Words Matrix:', df_bow_shape)\n",
    "print(df_bow.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenizing the processed text\n",
    "texts_tokenized = [word_tokenize(text) for text in df['proc_text'].values]\n",
    "\n",
    "# Training a Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=texts_tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Summarizing the model\n",
    "print('Word2Vec model trained.')\n",
    "print('Vocabulary size:', len(word2vec_model.wv.key_to_index))\n",
    "\n",
    "# Exploring word vectors\n",
    "example_word = 'panic'\n",
    "example_vector = word2vec_model.wv[example_word]\n",
    "print(f'Vector representation for \"{example_word}\":\\\n",
    "{example_vector}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
